{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa28e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee99acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n",
    "users = pd.read_csv(\"ml-1m/users.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n",
    "ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb5120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test set\n",
    "training_set = pd.read_csv(\"ml-100k/u1.base\", delimiter='\\t')\n",
    "training_set = np.array(training_set, dtype=\"int\")\n",
    "\n",
    "test_set = pd.read_csv(\"ml-100k/u1.test\", delimiter='\\t')\n",
    "test_set = np.array(test_set, dtype=\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff4d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting number the users and movies\n",
    "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b897b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into an array with users in lines and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b31123ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat Autoencoder class\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25563329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 1.0417044135646432\n",
      "Epoch: 2. Loss: 1.0320962157423146\n",
      "Epoch: 3. Loss: 1.0274428815882786\n",
      "Epoch: 4. Loss: 1.0241018331723175\n",
      "Epoch: 5. Loss: 1.0223813567008238\n",
      "Epoch: 6. Loss: 1.0208100095869381\n",
      "Epoch: 7. Loss: 1.0199032105208288\n",
      "Epoch: 8. Loss: 1.0187877103922682\n",
      "Epoch: 9. Loss: 1.0184962663079453\n",
      "Epoch: 10. Loss: 1.0176392498954334\n",
      "Epoch: 11. Loss: 1.0176858257819463\n",
      "Epoch: 12. Loss: 1.0171208261596916\n",
      "Epoch: 13. Loss: 1.0169368287235434\n",
      "Epoch: 14. Loss: 1.0165944462473813\n",
      "Epoch: 15. Loss: 1.0168461929211656\n",
      "Epoch: 16. Loss: 1.0162169726498305\n",
      "Epoch: 17. Loss: 1.0161963148903057\n",
      "Epoch: 18. Loss: 1.015902892921727\n",
      "Epoch: 19. Loss: 1.0161092227992725\n",
      "Epoch: 20. Loss: 1.0156673643100256\n",
      "Epoch: 21. Loss: 1.0160151234272536\n",
      "Epoch: 22. Loss: 1.0156044217268392\n",
      "Epoch: 23. Loss: 1.0152871200817652\n",
      "Epoch: 24. Loss: 1.014373305073074\n",
      "Epoch: 25. Loss: 1.012325289234618\n",
      "Epoch: 26. Loss: 1.0103684852948371\n",
      "Epoch: 27. Loss: 1.008054294575599\n",
      "Epoch: 28. Loss: 1.0046142107263472\n",
      "Epoch: 29. Loss: 1.0070653781680847\n",
      "Epoch: 30. Loss: 1.0015696214102952\n",
      "Epoch: 31. Loss: 0.9999380061043819\n",
      "Epoch: 32. Loss: 0.9976921707108296\n",
      "Epoch: 33. Loss: 0.9960457198740094\n",
      "Epoch: 34. Loss: 0.9948654019138268\n",
      "Epoch: 35. Loss: 0.9955225633996075\n",
      "Epoch: 36. Loss: 0.9929073053396489\n",
      "Epoch: 37. Loss: 0.9918575396398333\n",
      "Epoch: 38. Loss: 0.9898247075966178\n",
      "Epoch: 39. Loss: 0.9873417234868004\n",
      "Epoch: 40. Loss: 0.9841435071446908\n",
      "Epoch: 41. Loss: 0.9822943792313458\n",
      "Epoch: 42. Loss: 0.9800655845478606\n",
      "Epoch: 43. Loss: 0.9780548436288239\n",
      "Epoch: 44. Loss: 0.9774253662792166\n",
      "Epoch: 45. Loss: 0.9776641096775903\n",
      "Epoch: 46. Loss: 0.9716351240141365\n",
      "Epoch: 47. Loss: 0.9706144846874087\n",
      "Epoch: 48. Loss: 0.9676342940349025\n",
      "Epoch: 49. Loss: 0.9698230452572034\n",
      "Epoch: 50. Loss: 0.9683341049933682\n",
      "Epoch: 51. Loss: 0.9639121257222745\n",
      "Epoch: 52. Loss: 0.9630913966513376\n",
      "Epoch: 53. Loss: 0.9608705958048401\n",
      "Epoch: 54. Loss: 0.9572188737647623\n",
      "Epoch: 55. Loss: 0.9595818832624846\n",
      "Epoch: 56. Loss: 0.9580814365172472\n",
      "Epoch: 57. Loss: 0.9557010532338349\n",
      "Epoch: 58. Loss: 0.9547855799143407\n",
      "Epoch: 59. Loss: 0.954888715628711\n",
      "Epoch: 60. Loss: 0.9516701308020837\n",
      "Epoch: 61. Loss: 0.9519376614011228\n",
      "Epoch: 62. Loss: 0.9502500747751288\n",
      "Epoch: 63. Loss: 0.9525407029572407\n",
      "Epoch: 64. Loss: 0.9525063838406855\n",
      "Epoch: 65. Loss: 0.9474251724570952\n",
      "Epoch: 66. Loss: 0.9474039017351661\n",
      "Epoch: 67. Loss: 0.9475703998289429\n",
      "Epoch: 68. Loss: 0.9485058329187807\n",
      "Epoch: 69. Loss: 0.9474946448644362\n",
      "Epoch: 70. Loss: 0.9451568745805683\n",
      "Epoch: 71. Loss: 0.9454140282222384\n",
      "Epoch: 72. Loss: 0.9433471800114168\n",
      "Epoch: 73. Loss: 0.943537093010583\n",
      "Epoch: 74. Loss: 0.941330468148404\n",
      "Epoch: 75. Loss: 0.9416981938399624\n",
      "Epoch: 76. Loss: 0.940714317840289\n",
      "Epoch: 77. Loss: 0.940459251467915\n",
      "Epoch: 78. Loss: 0.9395964870622695\n",
      "Epoch: 79. Loss: 0.9400884976258312\n",
      "Epoch: 80. Loss: 0.9389346152506884\n",
      "Epoch: 81. Loss: 0.9390836228595861\n",
      "Epoch: 82. Loss: 0.9371261353487237\n",
      "Epoch: 83. Loss: 0.9389337476334675\n",
      "Epoch: 84. Loss: 0.9366853915227158\n",
      "Epoch: 85. Loss: 0.9379536013045132\n",
      "Epoch: 86. Loss: 0.9355455343157806\n",
      "Epoch: 87. Loss: 0.9370417452545479\n",
      "Epoch: 88. Loss: 0.9351835232325986\n",
      "Epoch: 89. Loss: 0.9365970225173073\n",
      "Epoch: 90. Loss: 0.9354017165463427\n",
      "Epoch: 91. Loss: 0.9379351021963832\n",
      "Epoch: 92. Loss: 0.9340719408173163\n",
      "Epoch: 93. Loss: 0.9353029854669661\n",
      "Epoch: 94. Loss: 0.9340685976178784\n",
      "Epoch: 95. Loss: 0.9346662928264126\n",
      "Epoch: 96. Loss: 0.9329128051929712\n",
      "Epoch: 97. Loss: 0.9334564900592071\n",
      "Epoch: 98. Loss: 0.9326588824832066\n",
      "Epoch: 99. Loss: 0.9333205608301474\n",
      "Epoch: 100. Loss: 0.9323074471571778\n",
      "Epoch: 101. Loss: 0.932935537454505\n",
      "Epoch: 102. Loss: 0.9316374329432184\n",
      "Epoch: 103. Loss: 0.9324116092205746\n",
      "Epoch: 104. Loss: 0.9320172297168802\n",
      "Epoch: 105. Loss: 0.9323315994454544\n",
      "Epoch: 106. Loss: 0.9318212771526241\n",
      "Epoch: 107. Loss: 0.931519426199688\n",
      "Epoch: 108. Loss: 0.9305173804790888\n",
      "Epoch: 109. Loss: 0.9312192098557771\n",
      "Epoch: 110. Loss: 0.9307278562661881\n",
      "Epoch: 111. Loss: 0.9309581070062746\n",
      "Epoch: 112. Loss: 0.930095274198942\n",
      "Epoch: 113. Loss: 0.9306465952099912\n",
      "Epoch: 114. Loss: 0.9308348846360184\n",
      "Epoch: 115. Loss: 0.9298284422369552\n",
      "Epoch: 116. Loss: 0.9296960361049337\n",
      "Epoch: 117. Loss: 0.9302043526372378\n",
      "Epoch: 118. Loss: 0.9293442110419111\n",
      "Epoch: 119. Loss: 0.9299051869871782\n",
      "Epoch: 120. Loss: 0.928784906507216\n",
      "Epoch: 121. Loss: 0.9293684966549302\n",
      "Epoch: 122. Loss: 0.9289055566642198\n",
      "Epoch: 123. Loss: 0.9289234057303415\n",
      "Epoch: 124. Loss: 0.9282398560314801\n",
      "Epoch: 125. Loss: 0.9283027209982287\n",
      "Epoch: 126. Loss: 0.9280998610958215\n",
      "Epoch: 127. Loss: 0.92760043654189\n",
      "Epoch: 128. Loss: 0.9276865072230223\n",
      "Epoch: 129. Loss: 0.9273388163077367\n",
      "Epoch: 130. Loss: 0.9271516738239002\n",
      "Epoch: 131. Loss: 0.9274178180978663\n",
      "Epoch: 132. Loss: 0.9268617693157976\n",
      "Epoch: 133. Loss: 0.9267524646195846\n",
      "Epoch: 134. Loss: 0.9265700513961262\n",
      "Epoch: 135. Loss: 0.9261235767507995\n",
      "Epoch: 136. Loss: 0.9263039259235402\n",
      "Epoch: 137. Loss: 0.9263207261908444\n",
      "Epoch: 138. Loss: 0.9257142932362704\n",
      "Epoch: 139. Loss: 0.9255946563214406\n",
      "Epoch: 140. Loss: 0.9251060476431278\n",
      "Epoch: 141. Loss: 0.9257025225418954\n",
      "Epoch: 142. Loss: 0.9253612815419553\n",
      "Epoch: 143. Loss: 0.9253431435841194\n",
      "Epoch: 144. Loss: 0.9250952721668162\n",
      "Epoch: 145. Loss: 0.9261495720522531\n",
      "Epoch: 146. Loss: 0.9252361266553429\n",
      "Epoch: 147. Loss: 0.9249612715937898\n",
      "Epoch: 148. Loss: 0.9242298624212067\n",
      "Epoch: 149. Loss: 0.9243032884930444\n",
      "Epoch: 150. Loss: 0.924072878946199\n",
      "Epoch: 151. Loss: 0.9243773912068851\n",
      "Epoch: 152. Loss: 0.9236764617087483\n",
      "Epoch: 153. Loss: 0.9237213424703941\n",
      "Epoch: 154. Loss: 0.9226451672057051\n",
      "Epoch: 155. Loss: 0.9228086012889247\n",
      "Epoch: 156. Loss: 0.922637750866977\n",
      "Epoch: 157. Loss: 0.9235198878849085\n",
      "Epoch: 158. Loss: 0.9225244818712588\n",
      "Epoch: 159. Loss: 0.9228137263651821\n",
      "Epoch: 160. Loss: 0.9217817551705056\n",
      "Epoch: 161. Loss: 0.9224829369688585\n",
      "Epoch: 162. Loss: 0.9216934054449224\n",
      "Epoch: 163. Loss: 0.9222589988722791\n",
      "Epoch: 164. Loss: 0.9211658102321577\n",
      "Epoch: 165. Loss: 0.9214460073039691\n",
      "Epoch: 166. Loss: 0.9212063426010874\n",
      "Epoch: 167. Loss: 0.9215991292611124\n",
      "Epoch: 168. Loss: 0.9201397636736198\n",
      "Epoch: 169. Loss: 0.9209487046628982\n",
      "Epoch: 170. Loss: 0.9203090897617701\n",
      "Epoch: 171. Loss: 0.9203370403086605\n",
      "Epoch: 172. Loss: 0.9197245973078078\n",
      "Epoch: 173. Loss: 0.920224212929542\n",
      "Epoch: 174. Loss: 0.9196851485367131\n",
      "Epoch: 175. Loss: 0.9200270597496671\n",
      "Epoch: 176. Loss: 0.9189251774202518\n",
      "Epoch: 177. Loss: 0.9193024156545335\n",
      "Epoch: 178. Loss: 0.9187769710647963\n",
      "Epoch: 179. Loss: 0.9193305235493137\n",
      "Epoch: 180. Loss: 0.9186779376106912\n",
      "Epoch: 181. Loss: 0.9190276592892463\n",
      "Epoch: 182. Loss: 0.9181934586145102\n",
      "Epoch: 183. Loss: 0.9187667532847563\n",
      "Epoch: 184. Loss: 0.9184136167991118\n",
      "Epoch: 185. Loss: 0.9185586528116644\n",
      "Epoch: 186. Loss: 0.9178030291086865\n",
      "Epoch: 187. Loss: 0.9182968050154339\n",
      "Epoch: 188. Loss: 0.9174431217169381\n",
      "Epoch: 189. Loss: 0.9178462630643672\n",
      "Epoch: 190. Loss: 0.917261534317947\n",
      "Epoch: 191. Loss: 0.9172420981376761\n",
      "Epoch: 192. Loss: 0.9168221137473416\n",
      "Epoch: 193. Loss: 0.9172269761876083\n",
      "Epoch: 194. Loss: 0.9163021831451834\n",
      "Epoch: 195. Loss: 0.9168803029854108\n",
      "Epoch: 196. Loss: 0.9158814041996947\n",
      "Epoch: 197. Loss: 0.9161898082475826\n",
      "Epoch: 198. Loss: 0.9160388956883713\n",
      "Epoch: 199. Loss: 0.9162856065511144\n",
      "Epoch: 200. Loss: 0.9157166404073983\n"
     ]
    }
   ],
   "source": [
    "# Train SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "            s += 1\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"Epoch: {}. Loss: {}\".format(epoch, train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da4f20ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9498989074811025\n"
     ]
    }
   ],
   "source": [
    "# Testing SAE perfomance\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "        s += 1\n",
    "\n",
    "print(\"Test Loss: {}\".format(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLEnv_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
